# Multi-Modal Open-Vocabulary Grasping
Object grasping in robotics is inherently challenging due to the need for higher-level understanding. Instead of relying on basic commands like "grasp object at Pose X," robots must interpret more complex instructions, such as "grasp the apple." Moreover, these systems should not be confined to a fixed set of objects seen during training; they must be capable of operating in an open-vocabulary manner. By incorporating multi-modal capabilities, robots can engage with humans more dynamically, accepting inputs such as text, images, and audio. This project, Multi-Modal Open-Vocabulary Grasping, integrates foundational models for scene understanding with Graspnet for grasp prediction, allows robots to interact intelligently and flexibly with their environment, respond to high-level queries, and enhance human-robot interaction in dynamic environments.

<div align="center">
  <img src="images/workflow.png" alt="openvocabgrasp" width="50%">
</div>